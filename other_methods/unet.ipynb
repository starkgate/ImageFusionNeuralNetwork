{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"D:/BELEG/\"\n",
    "# contains *.tif, *_MRI.nii, *_LABEL.nii, *_GT.png files\n",
    "path_training = base_path + \"Dataset/training-cropped-masked/\"\n",
    "path_testing = base_path + \"Dataset/testing-cropped-masked/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.framework import constant_op, dtypes, ops\n",
    "from tensorflow.python.ops import array_ops, control_flow_ops, math_ops, nn, nn_ops\n",
    "import os\n",
    "\n",
    "import imageio\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import backend as keras\n",
    "\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import ipywidgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "# define parameters\n",
    "batch_size = 1\n",
    "epoch = 30\n",
    "lr = 0.002\n",
    "lamda = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def save_png(tensor, name):\n",
    "    img = tf.cast(img, tf.uint8)\n",
    "    img = tf.image.encode_png(img)\n",
    "    tf.io.write_file('pred/' + name, img)\n",
    "    return tensor\n",
    "\n",
    "def show_volume(volume):\n",
    "  show_information(volume)\n",
    "  a = tf.transpose(volume,(2,0,1))\n",
    "  def print_volume(x):\n",
    "    plt.imshow(a[x], cmap='jet')\n",
    "  interact(print_volume, x=ipywidgets.IntSlider(min=0, max=tf.shape(volume)[2]-1, step=1, value=0))\n",
    "\n",
    "def show_information(x):\n",
    "  tf.print(\"Tensor Shape:\", tf.shape(x))\n",
    "  tf.print(\"Shape:\", x.shape)\n",
    "  tf.print(\"Mean:\", tf.reduce_mean(x))\n",
    "  tf.print(\"Min:\", tf.reduce_min(x))\n",
    "  tf.print(\"Max:\", tf.reduce_max(x))\n",
    "\n",
    "@tf.function\n",
    "def normalize_dataset(tensor):\n",
    "    return tf.divide(\n",
    "       tf.subtract(\n",
    "          tensor,\n",
    "          tf.reduce_min(tensor)\n",
    "       ),\n",
    "       tf.subtract(\n",
    "          tf.reduce_max(tensor),\n",
    "          tf.reduce_min(tensor)\n",
    "       )\n",
    "    )\n",
    "  \n",
    "@tf.function\n",
    "def to_float_dataset(tensor):\n",
    "    return tf.cast(tensor, tf.float32)\n",
    "\n",
    "@tf.function\n",
    "def zero_to_one_dataset(tensor):\n",
    "    # int16 to float32, scaling from 0-32k to 0-1 is done automatically\n",
    "   return tf.image.convert_image_dtype(tensor, tf.float32)\n",
    "\n",
    "# loss function : 2 possibilities\n",
    "# SSIM on prediction vs NSST groundtruth\n",
    "# or SSIM on prediction vs each of the input slices\n",
    "# y_true is the input volume\n",
    "@tf.function\n",
    "def loss_ssim_unsupervised(y_true, y_pred):\n",
    "    #shape pred [1 240x 240y 1z]\n",
    "    #shape true [1 240x 240y 155z]\n",
    "    y_true = tf.transpose(y_true, perm=[3,1,2,0])\n",
    "    y_pred = tf.squeeze(y_pred, 0)\n",
    "    #reshape pred [240x 240y 1z]\n",
    "    #reshape true [155z 240x 240y 1]\n",
    "\n",
    "    def ssim_l1_loss(elems):\n",
    "        ssim_layer_loss = lamda * (1 - tf.image.ssim(elems, y_pred, max_val = 1))\n",
    "        #tf.print(ssim_layer_loss)\n",
    "        return ssim_layer_loss\n",
    "\n",
    "    loss = tf.map_fn(ssim_l1_loss, y_true, dtype=tf.float32)\n",
    "    # sum of all (l1 + ssim) layer losses = total loss\n",
    "    return tf.reduce_sum(loss)\n",
    "    \n",
    "@tf.function\n",
    "def loss_binary_crossentropy_unsupervised(y_true, y_pred):\n",
    "    y_true = tf.transpose(y_true, perm=[3,1,2,0])\n",
    "    y_pred = tf.squeeze(y_pred, 0)\n",
    "\n",
    "    def binary_crossentropy_l1_loss(elems):\n",
    "        binary_crossentropy_layer_loss = lamda * (1 - tf.keras.backend.binary_crossentropy(elems, y_pred))\n",
    "        return binary_crossentropy_layer_loss\n",
    "\n",
    "    loss = tf.map_fn(binary_crossentropy_l1_loss, y_true, dtype=tf.float32)\n",
    "    return tf.reduce_sum(loss)\n",
    "\n",
    "# SSIM on prediction vs groundtruth\n",
    "# y_true is the NSST groundtruth\n",
    "@tf.function\n",
    "def loss_ssim_supervised_nsst(y_true, y_pred):\n",
    "    # ssim and l1 loss : compare NSST groundtruth to the network's prediction\n",
    "    ssim_mri_loss = lamda * (1 - tf.image.ssim(y_pred, y_true, max_val = 1.0))\n",
    "    l1_mri_loss = (1 - lamda) * (tf.reduce_mean(tf.abs(y_pred - y_true)))\n",
    "    return ssim_mri_loss + l1_mri_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PREPARING DATASET\")\n",
    "# load the training files from img folder : 3D MRI, 3D label mask\n",
    "get_volume = lambda file: nib.load(file).get_data()\n",
    "get_image = lambda file: pyplot.imread(file)\n",
    "\n",
    "# 100 volumes, 100x240x240x155 voxels, values in 0-2^16/2, int16 array\n",
    "train_volumes_mri = np.array(list(map(get_volume, glob.glob(path_training + \"*.nii\"))))\n",
    "print(\"Found training dataset\")\n",
    "# implicit cast to float32 tensor, values in 0-1\n",
    "# 3 options: normalize or divide by max integer value or cast to float\n",
    "train_volumes_mri = tf.map_fn(normalize_dataset, train_volumes_mri, dtype=tf.float32)\n",
    "print(\"Normalized training dataset\")\n",
    "\n",
    "test_volumes_mri = np.array(list(map(get_volume, glob.glob(path_testing + \"*.nii\"))))\n",
    "print(\"Found testing dataset\")\n",
    "# normalize the data\n",
    "test_volumes_mri = tf.map_fn(normalize_dataset, test_volumes_mri, dtype=tf.float32)\n",
    "print(\"Normalized testing dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.test.is_gpu_available())\n",
    "show_information(train_volumes_mri[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape=(128,128,128))\n",
    "conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(input)\n",
    "conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(pool1)\n",
    "conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(conv2)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(pool2)\n",
    "conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(conv3)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(pool3)\n",
    "conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(conv4)\n",
    "pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(pool4)\n",
    "conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(conv5)\n",
    "\n",
    "up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(UpSampling2D(size = (2,2))(conv5))\n",
    "merge6 = concatenate([conv4,up6], axis = 3)\n",
    "conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(merge6)\n",
    "conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(conv6)\n",
    "\n",
    "up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(UpSampling2D(size = (2,2))(conv6))\n",
    "merge7 = concatenate([conv3,up7], axis = 3)\n",
    "conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(merge7)\n",
    "conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(conv7)\n",
    "\n",
    "up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(UpSampling2D(size = (2,2))(conv7))\n",
    "merge8 = concatenate([conv2,up8], axis = 3)\n",
    "conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(merge8)\n",
    "conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(conv8)\n",
    "\n",
    "up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(UpSampling2D(size = (2,2))(conv8))\n",
    "merge9 = concatenate([conv1,up9], axis = 3)\n",
    "conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(merge9)\n",
    "conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(conv9)\n",
    "network = Conv2D(1, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', data_format = 'channels_last')(conv9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DEFINING MODEL\")\n",
    "model = tf.keras.Model(inputs = input, outputs = network)\n",
    "#model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.002), loss = loss_binary_crossentropy_unsupervised, metrics = ['accuracy'])\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.002), loss = loss_ssim_unsupervised, metrics = ['accuracy'])\n",
    "\n",
    "checkpoint_path = base_path + \"tfgan/checkpoints-unet/checkpoint-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1)\n",
    "latest = tf.train.latest_checkpoint(base_path + \"tfgan/checkpoints-unet\")\n",
    "if(latest == None):\n",
    "    print(\"No checkpoints\")\n",
    "else:\n",
    "    print(\"Loading latest checkpoint\")\n",
    "    model.load_weights(latest)\n",
    "#model.summary(line_length=100)\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=base_path + 'logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class save_epoch_prediction(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        predictions = self.model.predict(test_volumes_mri, batch_size = 1, verbose = 1)\n",
    "        i=0\n",
    "        for pred in predictions:\n",
    "            img = tf.image.convert_image_dtype(pred, tf.uint8)\n",
    "            img = tf.image.encode_png(img)\n",
    "            name = base_path + 'tfgan/checkpoints-unet/pred/' + str(i) + '.png'\n",
    "            tf.io.write_file(name, img)\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STARTING TRAINING PHASE\")\n",
    "# train\n",
    "history_fit = model.fit(train_volumes_mri, train_volumes_mri, epochs = epoch, batch_size = batch_size, callbacks = [cp_callback, save_epoch_prediction()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STARTING TESTING PHASE\")\n",
    "show_information(test_volumes_mri)\n",
    "history_eval = model.evaluate(test_volumes_mri, test_volumes_mri, batch_size = batch_size)\n",
    "\n",
    "# get results\n",
    "predictions = model.predict(test_volumes_mri, batch_size = 1, verbose = 1)\n",
    "print(\"max pixel value:\", tf.reduce_max(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.print(\"predictions\")\n",
    "show_information(predictions)\n",
    "tf.print(\"\\ntrain-dataset\")\n",
    "show_information(train_volumes_mri)\n",
    "tf.print(\"\\ntest-dataset\")\n",
    "show_information(test_volumes_mri)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
